{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kenlm\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from string import punctuation\n",
    "from collections import defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Candidate Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_one_edit_distance(word):\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def generate_two_edit_distance(word):\n",
    "    return set(e2 for e1 in generate_one_edit_distance(word) for e2 in generate_one_edit_distance(e1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram\n",
    "\n",
    "Our calculation for unigram does not use any smoothing, since we argue the exact probabilities of each words does not matter (we only care about which words is more probable than others). We do not use the KenLM for unigram since the author only implement it for bigram and higher order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./ind_news_2022_1M-sentences.txt\"\n",
    "data = []\n",
    "with open(path, \"r\", encoding=\"UTF-8\") as file:\n",
    "    i = 0\n",
    "    for line in file:\n",
    "        # print(\"Iteration\", str(i), \": \", line)\n",
    "        i = i + 1;\n",
    "        data.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_tokenize(data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sentence):\n",
    "  words = word_tokenize(sentence)\n",
    "  processed_words = []\n",
    "  for word in words:\n",
    "    if word.isnumeric():\n",
    "      processed_words.append(\"n4m3r1c\")\n",
    "    elif word not in punctuation and word != \"''\" and word != \"“\" and word != \"”\":\n",
    "      processed_words.append(word.lower())\n",
    "  return processed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_freq = dict()\n",
    "\n",
    "for line in data:\n",
    "\tlist_words = preprocess_text(line)\n",
    "\tfor word in list_words:\n",
    "\t\tif word in unigram_freq.keys():\n",
    "\t\t\tunigram_freq[word] += 1\n",
    "\t\telse:\n",
    "\t\t\tunigram_freq[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_unigram(word_list):\n",
    "\tsorted_list = []\n",
    "\tfor word in word_list:\n",
    "\t\tif word in unigram_freq.keys():\n",
    "\t\t\tsorted_list.append((word, unigram_freq[word]))\n",
    "\t\telse:\n",
    "\t\t\tsorted_list.append((word, 0))\n",
    "\tsorted_list.sort(key=lambda x: x[1], reverse=True)\n",
    "\treturn sorted_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram and Trigram Model\n",
    "\n",
    "We use KenLM to help us to compute the bigram and trigram probabilities with Knesser-Ney smoothing. We already train the model using 1 million sentence from Indonesian News."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bigram_sorter:\n",
    "  def __init__(self):\n",
    "    self.model_bigram = kenlm.Model(\"./cleaned_corpus_bigram.klm\")\n",
    "\n",
    "  def get_sorted_list(self, preceeding_sentence, word_list, following_sentence):\n",
    "    sorted_list = []\n",
    "    for word in word_list:\n",
    "      new_sentence = preceeding_sentence+\" \"+word+\" \"+following_sentence\n",
    "      sorted_list.append((word, self.model_bigram.score(new_sentence)))\n",
    "    sorted_list.sort(key=lambda x: x[1], reverse=True)\n",
    "    return sorted_list\n",
    "\n",
    "\n",
    "class trigram_sorter:\n",
    "  def __init__(self):\n",
    "    self.model_trigram = kenlm.Model(\"./cleaned_corpus_trigram.klm\")\n",
    "\n",
    "  def get_sorted_list(self, preceeding_sentence, word_list, following_sentence):\n",
    "    sorted_list = []\n",
    "    for word in word_list:\n",
    "      new_sentence = preceeding_sentence+\" \"+word+\" \"+following_sentence\n",
    "      sorted_list.append((word, self.model_trigram.score(new_sentence)))\n",
    "    sorted_list.sort(key=lambda x: x[1], reverse=True)\n",
    "    return sorted_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_list = bigram_sorter()\n",
    "sorted_list.get_sorted_list(\"aku\", [\"pntar\", \"pintar\", \"pantir\", \"petir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./list_kbbi.txt\"\n",
    "kbbi_words = set()\n",
    "with open(path, \"r\") as file:\n",
    "    for i, line in enumerate(file):\n",
    "        # print(\"Iteration\", str(i), \": \", line)\n",
    "        words = line.rstrip('\\n').split(' ')\n",
    "        for word in words:\n",
    "          if(word != \"\" and word.isalpha()):\n",
    "            kbbi_words.add(str(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"n4m3r1c hai\"\n",
    "words = word_tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spell Corrector Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Spell_Corrector:\n",
    "\tdef __init__(self):\n",
    "\t\tself.bigram = bigram_sorter()\n",
    "\t\tself.trigram = trigram_sorter()\n",
    "\n",
    "\tdef generate_correction(self, sentence):\n",
    "\t\tsentence = sentence.lower()\n",
    "\t\twords = word_tokenize(sentence)\n",
    "\n",
    "\t\tlist_unigram = []\n",
    "\t\tlist_bigram = []\n",
    "\t\tlist_trigram = []\n",
    "\n",
    "\t\tfor index, word in enumerate(words):\n",
    "\t\t\tstemmed_word = stemmer.stem(word)\n",
    "\t\t\t# check if this word is non-word error\n",
    "\t\t\tif stemmed_word not in kbbi_words and word != \"n4m3r1c\" and word not in punctuation:\n",
    "\t\t\t\t# not exists in KBBI\n",
    "\t\t\t\tpossible_corrections = generate_one_edit_distance(word)\n",
    "\t\t\t\tpossible_corrections = possible_corrections.union(generate_two_edit_distance(word))\n",
    "\t\t\t\tfiltered_corrections = []\n",
    "\t\t\t\t\n",
    "\t\t\t\t# filter possible word corrections using KBBI\n",
    "\t\t\t\tfor possible_word in possible_corrections:\n",
    "\t\t\t\t\tif possible_word in kbbi_words:\n",
    "\t\t\t\t\t\tfiltered_corrections.append(possible_word)\n",
    "\n",
    "\t\t\t\t# sort the list based on the N-gram probability\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Use unigram\n",
    "\t\t\t\tsorted_list_unigram = get_sorted_unigram(filtered_corrections)\n",
    "\t\t\t\tlist_unigram.append(sorted_list_unigram[:5])\n",
    "\n",
    "\t\t\t\t# Use bigram\n",
    "\t\t\t\tsorted_list_bigram = self.bigram.get_sorted_list(\n",
    "\t\t\t\t\t(words[index-1] if index > 0 else \"\"), \n",
    "\t\t\t\t\tfiltered_corrections, \n",
    "\t\t\t\t\t(words[index+1] if index < len(words)-1 else \"\"))\n",
    "\t\t\t\tlist_bigram.append(sorted_list_bigram[:5])\n",
    "\n",
    "\t\t\t\t# Use trigram\n",
    "\t\t\t\tsorted_list_trigram = self.trigram.get_sorted_list(\n",
    "\t\t\t\t\t(words[index-2] if index > 1 else \"\") + (words[index-1] if index > 0 else \"\"), \n",
    "\t\t\t\t\tfiltered_corrections,\n",
    "\t\t\t\t\t(words[index+2] if index < len(words)-2 else \"\") + (words[index+1] if index < len(words)-1 else \"\"))\n",
    "\t\t\t\tlist_trigram.append(sorted_list_trigram[:5])\n",
    "\t\t\t\n",
    "\t\t\telse:\n",
    "\t\t\t\tlist_unigram.append([])\n",
    "\t\t\t\tlist_bigram.append([])\n",
    "\t\t\t\tlist_trigram.append([])\n",
    "\n",
    "\t\treturn list_trigram\n",
    "\t\t\t\n",
    "\n",
    "\tdef print_correction(self, sentence):\n",
    "\t\tunigram_corr, bigram_corr, trigram_corr = self.generate_correction(sentence)\n",
    "\t\tsentence = sentence.lower()\n",
    "\t\twords = word_tokenize(sentence)\n",
    "\t\tfor i, word in enumerate(words):\n",
    "\t\t\tprint(f\"Correction for {word}\")\n",
    "\n",
    "\t\t\tprint(\"Unigram: \")\n",
    "\t\t\tprint(unigram_corr[i])\n",
    "\n",
    "\t\t\tprint(\"Bigram: \")\n",
    "\t\t\tprint(bigram_corr[i])\n",
    "\n",
    "\t\t\tprint(\"Trigram: \")\n",
    "\t\t\tprint(trigram_corr[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_spell_checker = Spell_Corrector()\n",
    "id_spell_checker.print_correction(\"indonesia memiliki sumbr daya alam yang mllimpah\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_path = \"./cleaned_test.txt\"\n",
    "data = []\n",
    "with open(testing_path, \"r\", encoding=\"UTF-8\") as file:\n",
    "    i = 0\n",
    "    for line in file:\n",
    "        # print(\"Iteration\", str(i), \": \", line)\n",
    "        i = i + 1;\n",
    "        data.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Typo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 0.3\n",
    "legit_sents = []\n",
    "typo_sents = []\n",
    "for sentence in data:\n",
    "\twords = sentence.split(\" \")\n",
    "\tlegit_words = []\n",
    "\ttypo_words = []\n",
    "\tfor word in words:\n",
    "\t\trand = random.random()\n",
    "\t\tif word == \"\" or word == \"\\n\" or word == \" \":\n",
    "\t\t\tcontinue\n",
    "\t\tlegit_words.append(word)\n",
    "\t\tif rand <= P and word != \"n4m3r1c\":\n",
    "\t\t\tedit_distance = random.randint(1, 2)\n",
    "\n",
    "\t\t\tif edit_distance == 1:\n",
    "\t\t\t\tword_cand = list(generate_one_edit_distance(word))\n",
    "\t\t\t\ttypo_words.append(word_cand[random.randint(0, len(word_cand)-1)])\n",
    "\t\t\telse:\n",
    "\t\t\t\tword_cand1 = list(generate_one_edit_distance(word))\n",
    "\t\t\t\tword_cand2 = list(generate_one_edit_distance(word_cand1[random.randint(0, len(word_cand1)-1)]))\n",
    "\t\t\t\ttypo_words.append(word_cand2[random.randint(0, len(word_cand2)-1)])\n",
    "\t\telse:\n",
    "\t\t\ttypo_words.append(word)\n",
    "\tnew_sentence = \"\"\n",
    "\tfor word in typo_words:\n",
    "\t\tnew_sentence += word + \" \"\n",
    "\ttypo_sents.append(new_sentence)\n",
    "\n",
    "\tnew_sentence = \"\"\n",
    "\tfor word in legit_words:\n",
    "\t\tnew_sentence += word + \" \"\n",
    "\tlegit_sents.append(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"typo_test.txt\", \"w\", encoding=\"UTF-8\")\n",
    "for sentence in new_data:\n",
    "\tf.write(sentence + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_typo = 0\n",
    "# accuracy for 3, 4, and 5 top list\n",
    "unigram_acc = [0, 0, 0]\n",
    "bigram_acc = [0, 0, 0]\n",
    "trigram_acc = [0, 0, 0]\n",
    "\n",
    "spell_corrector = Spell_Corrector()\n",
    "\n",
    "cnt = 1\n",
    "\n",
    "for (real_sentence, typo_sentence) in zip(legit_sents, typo_sents):\n",
    "\t# print(real_sentence)\n",
    "\t# print(typo_sentence)\n",
    "\n",
    "\tcnt += 1\n",
    "\n",
    "\tif cnt % 500 == 0:\n",
    "\t\tprint(cnt)\n",
    "\n",
    "\treal_words = word_tokenize(real_sentence)\n",
    "\ttypo_words = word_tokenize(typo_sentence)\n",
    "\tunigram_lists, bigram_lists, trigram_lists = spell_corrector.generate_correction(typo_sentence)\n",
    "\tfor idx, (real_word, typo_word, unigram_list, bigram_list, trigram_list) in enumerate(zip(real_words, typo_words, unigram_lists, bigram_lists trigram_lists)):\n",
    "\t\tif real_word == typo_word:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tunigram_list = [uni[0] for uni in unigram_list]\n",
    "\t\tbigram_list = [bi[0] for bi in bigram_list]\n",
    "\t\ttrigram_list = [tri[0] for tri in trigram_list]\n",
    "\n",
    "\n",
    "\t\ttotal_typo += 1\n",
    "\n",
    "\t\tif real_word in unigram_list[:min(3, len(unigram_list))]:\n",
    "\t\t\tunigram_acc[0] += 1\n",
    "\t\t\n",
    "\t\tif real_word in unigram_list[:min(4, len(unigram_list))]:\n",
    "\t\t\tunigram_acc[1] += 1\n",
    "\t\t\n",
    "\t\tif real_word in unigram_list[:min(5, len(unigram_list))]:\n",
    "\t\t\tunigram_acc[2] += 1\n",
    "\n",
    "\t\tif real_word in bigram_list[:min(3, len(bigram_list))]:\n",
    "\t\t\tbigram_acc[0] += 1\n",
    "\t\t\n",
    "\t\tif real_word in bigram_list[:min(4, len(bigram_list))]:\n",
    "\t\t\tbigram_acc[1] += 1\n",
    "\t\t\n",
    "\t\tif real_word in bigram_list[:min(5, len(bigram_list))]:\n",
    "\t\t\tbigram_acc[2] += 1\n",
    "\t\t\n",
    "\t\tif real_word in trigram_list[:min(3, len(trigram_list))]:\n",
    "\t\t\ttrigram_acc[0] += 1\n",
    "\t\t\n",
    "\t\tif real_word in trigram_list[:min(4, len(trigram_list))]:\n",
    "\t\t\ttrigram_acc[1] += 1\n",
    "\t\t\n",
    "\t\tif real_word in trigram_list[:min(5, len(trigram_list))]:\n",
    "\t\t\ttrigram_acc[2] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_typo)\n",
    "for i in range(3):\n",
    "\tprint(f\"Unigram Top {i+3} List: {unigram_acc[i]/total_typo}\")\n",
    "\n",
    "for i in range(3):\n",
    "\tprint(f\"Bigram Top {i+3} List: {bigram_acc[i]/total_typo}\")\t\n",
    "\n",
    "for i in range(3):\n",
    "\tprint(f\"Trigram Top {i+3} List: {trigram_acc[i]/total_typo}\")\n",
    "\n",
    "print((unigram_acc[0]+unigram_acc[1]+unigram_acc[2])/(3*total_typo))\n",
    "print((bigram_acc[0]+bigram_acc[1]+bigram_acc[2])/(3*total_typo))\n",
    "print((trigram_acc[0]+trigram_acc[1]+trigram_acc[2])/(3*total_typo))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
